\documentclass[10pt,epsf]{article}
\usepackage{amssymb,amsmath,amsthm,amsfonts,mathrsfs,color}
\usepackage{mathtools,commath}
\usepackage{epsfig}
\usepackage{latexsym}
\usepackage{verbatim}
\usepackage{setspace}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{algorithmicext}
\usepackage{ifthen}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{longtable}
\usepackage{url}
\usepackage{hyperref}
\usepackage[utf8]{luainputenc}
\usepackage[bibencoding=utf8,backend=biber]{biblatex}
\addbibresource{cosc6342-hw2-michael-yantosca.bib}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{{\footnotesize{COSC6342 HW 2}}}
\rhead{{\footnotesize{Michael Yantosca}}}

\newtheorem{fact}{Fact}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{observation}{Observation}
\newtheorem{exercise}{Exercise}
\newtheorem{statement}{Statement}
\newtheorem{problem}{Problem}

\newcommand{\TODO}[0]{\textbf{\color{red}{TODO}}}
\newcommand{\UNFINISHED}[0]{\textbf{\color{orange}{UNFINISHED}}}
\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\nseq}[1]{#1_{1}, \dots, #1_{n}}
\newcommand{\nnseq}[1]{#1_{1}, #1_{2}, \dots, #1_{n}}
\newcommand{\nseqeq}[1]{#1 = 1, \dots, n }
\newcommand{\nnseqeq}[1]{#1 = 1, 2, \dots, n }
\newcommand{\N}[2]{\mathcal{N}(#1, #2)}
\newcommand{\mxn}[1]{\mathbf{#1}}
\newcommand{\erfc}[1]{\mathbf{\Phi}(#1)}
\DeclareMathOperator*{\argmax}{arg\,max}

\everymath{\textstyle}

\usepgfplotslibrary{external}
\usepgfplotslibrary{statistics}
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{pgfplots.groupplots, external, patterns}
\tikzexternalize[]
\pgfplotsset{
  tick label style={font=\footnotesize},
  label style={font=\small},
  legend style={font=\small},
  compat=newest
}

\date{}
\title{Is Amplitude All You Need?}
\author{Michael Yantosca}
\begin{document}
\maketitle
\tableofcontents
\abstract{}{
}
\section{Introduction}{
  The past decade in natural language processing (NLP) research has seen a trend of
  eliminating complex intermediate stages in models in favor of deep-learning approaches
  that translate input data directly to the desired output. Salient examples in this vein include
  ``Deep Speech''\autocite{deepspeech}; ``Listen, Attend and Spell''\autocite{LAS};
  and most recently, ``Attention Is All You Need''\autocite{AIAYN}. While the former two
  rely on recurrent neural networks to avoid phonemic representations or costly
  hidden Markov models (HMMs), the last dispenses with recurrence and convolutions
  entirely, asserting that a simplified attention mechanism is all that is required
  to achieve competitive results.

  These intermediate state reductions have been directed primarily at the models
  which have been heretofore the province of linguistic theory. If the issue is
  mostly a matter of linear transformation, the question then arises as to how
  minimally one can preprocess the acoustic data and still make reasonable predictions.

  In the research mentioned above, acoustic input data typically takes the form of
  spectrograms\autocite[2]{deepspeech} and filter-bank spectra features\autocite[2]{LAS}.
  The Transformer architecture in the last paper deals strictly with textual sequence-to-sequence
  translation as opposed to speech recognition, but the minimalist principles cohere with
  the other work, and the removal of recurrence and the published results are worth mentioning.

  Since spectrograms and similar data representations are themselves simply convolutions
  of the raw amplitude data, one could theoretically apply the same minimalist principles in
  the opposite direction and use windowed batches of raw amplitudes as direct input to
  a neural network. This work measures the feasibility of such an approach applied to
  English phoneme classification. In keeping with the theme of simplicity, the neural
  network constructed solely uses backpropagation and stochastic gradient descent for training.
}

\section{Design and Implementation}{
  The complexities of splitting continuous audio data into phonemic frames is glossed
  over in the interest of time. The training and test sets come from the University
  of East Anglia time series classification archive\autocite{ueamvtsca}\autocite{Phoneme},
  which provided in the Phoneme challenge a small subset of the 370,000 labeled phoneme frames originally
  published by Hamooni and Mueen\autocite{DDHCPTS}. The frames are normalized as a series
  of 1,024 amplitude values followed by a numeric phoneme label.

  Notably, the Phoneme challenge currently reports a maximum achieved accuracy of 30.28\%.
  This may be due to the fact that the training set (214 examples) is nearly an order of magnitude smaller than
  the test set (1,896 examples), and data-hungry techniques such as neural networks are likely to fare
  poorly in the context of the challenge. However, the uniformity of the data layout provided in easily
  digestible ARFFs\autocite{scipyloadarff} present a manageable environment for an initial foray in
  evaluating the usefulness of training a neural network solely on amplitude values with only
  minor, inexpensive preprocessing.

  The Python program written to execute the experiments is dubbed the
  Parameterized Interactive Neural Network Iteratively Plotting Experimental Decisions, or PINNIPED for short.
  PINNIPED is built on PyTorch\autocite{torchnnref}\autocite{torchnntut},
  leveraging the framework's autograd feature, and can operate in either training mode or test mode.
  The user may specify a number of parameters to minutely specify the neural network to be trained,
  including non-linear activation, learning rate, learning momentum, batch size, training epochs,
  number of hidden layers and nodes within each layer individually, and input test or training set ARFFs.

  Users are encouraged to review the included \texttt{README.md} or the program's help message
  for details on usage.

  \subsection{Neural Network Layout}{
    The neural network composed by user-specification at the command line is a simple feed-forward
    network with backpropagation\autocite[284-296]{DHS}. The counts of nodes per layer is specified
    through the \texttt{--layer-dims} command-line argument as a comma-separated list of integer values.
    As such, the number of hidden layers permitted is arbitrary, but this work focuses on the single
    hidden-layer case. At least 2 layers, i.e., input and output, must be specified. The layer itself
    is implemented as a PyTorch Linear module.

    The same non-linear activation function is applied to the output of all hidden layers. The user
    may choose between the sigmoid, hyperbolic tangent, or rectified linear unit (ReLU). The non-linear
    transformation is implemented through the corresponding Pytorch module.
  }

  \subsection{Training}{
    Training may be parameterized on the command line according to a few different typical regimens.
    For the purpose of validation, the user may reserve either the last fraction of the training samples
    (e.g., the last quarter) or samples taken at regular intervals (e.g., every fourth sample).
    PINNIPED supports both batch backpropagation and stochastic gradient descent (SGD), but it does not
    support online training.

    Under SGD, the order of the training samples is permuted before training starts\autocite{torchshuffle},
    and reservation is done \emph{after} this initial permutation. No permutation of sample order is done
    if not specified by the user. The division betwen training and validation sets holds for the duration
    of training, but SGD also permutes the training set sample order at the start of each epoch so that
    each training sample is seen once per epoch but in a potentially different order each time.
    The validation set order is not permuted after reservation since no learning is taking place during
    validation, i.e., the PyTorch autograd mechanism is turned off for testing.

    Batching can be done in batches of 1 up to the total training set size. Specifying a number greater
    than the training set size will default to the training set size. If the speificied batch size does
    not divide the training set evenly, the last batch will be smaller than this specification. Users who
    require exact equality in size for all batches must either truncate the training set or augment it
    with additional samples if the set is not evenly divisible by the batch size.

    In order to make it easier to plot training, validation, and test accuracy in
    one graph, the test set may be passed through as a sort of second validation set. With intermediate
    plotting turned on, this will generate the performance of the model at every training stage against
    all the sets. This is not recommended for development testing since it can overfit to the
    test set and furthermore violates the best practices of test-set blinding that have been established
    in the field of machine learning so as to allow each model to be judged on its own merits and not the furtive
    hand of its inventor. However, the option is left as a convenience for illustrative and didactic purposes.
  }
  \subsection{Artifacts}{
    A number of artifacts can be generated with user-specified regularity, namely plots of performance
    and evolution along with intermediate model states.

    Using matplotlib\autocite{mplpplot} library for generation, the plots include the following:
    \begin{itemize}
    \item{a plot of training, validation, and test accuracy over time (epochs)}
    \item{per-layer heatmaps of weight vector changes by angle and norm per node over time (epochs)}
    \item{per-layer heatmaps of non-linear activation counts of nodes against uniform bins}
    \item{heatmaps of training, validation, and test confusion matrices}
    \end{itemize}

    All plots are rendered in PNG format. Heatmaps are generated as \texttt{pcolormesh}\autocite{mplppcolormesh}
    rather than \texttt{imshow}\autocite{mplpimshow} plots because the latter do not visually scale as well.
    The color values used by the heatmaps belong to the 'hot' scale\autocite{mplpcolorbar}, which seemed to be the most visually intuitive representation in all the use cases above.

    The weight angle change graph uses the standard formula for angle difference between two vectors\autocite[335]{Lay},
    \begin{align*}
      \mxn{u} \cdot \mxn{v} &= \norm{\mxn{u}} \norm{\mxn{v}} \cos \theta \\
      \theta &= \arccos \bigg[ \frac{\mxn{u} \cdot \mxn{v}}{ \norm{\mxn{u}} \norm{\mxn{v}} } \bigg]
    \end{align*}
    where $\theta$ is the angle between the two vectors $\mxn{u}$ and $\mxn{v}$. Because of documented
    concerns over precision in PyTorch\autocite{torchbug8069}, additional precautions were taken in
    the boundary cases close to the extrema of the $\arccos$ range. Additionally, colinear weight vectors,
    i.e., vectors with an angle of 0 radians between them, may not have the same magnitude, hence, the
    change in norm plot. If either vector had zero length, the change in angle was clamped to zero on
    the premise that zero vectors are colinear with all vectors, so to speak.

    The intermediate model states are saved as Pickle serializations of the PyTorch model state dictionaries.
    This binary format for Python object serialization can be imported for test-only runs of the program.
    The intermediate activation values are captured by a forward hook\autocite{torchactivations}
    which increments a tensor\autocite{torchtensors} of bins. Because none of the modules themselves
    have unique name properties, currying\autocite{functools} was employed to associate a common hook
    wrapped in a partial lambda to avoid code duplication.

    If the user turns on debug output, most of the actual values used to generate the plots will be printed
    to \texttt{stderr}. The confusion matrices are printed in this mode with a sparse text layout blanking
    out cells holding zeros for legibility as well as prefixing hits with a plus (+) and misses with a minus (-).
  }
  \subsection{Testing}{
    PINNIPED can run in a test-only mode, which is useful for evaluating a model against multiple test sets
    and avoiding the time penalty of repeating the same training multiple times. Of course, it should be noted
    that SGD training runs are not guaranteed to be perfectly reproducible.

    The test-only mode does not yield any artifacts other than the output to stderr of the accuracy. No
    additional models are saved nor any plots made. The latter would be a worthwhile enhancement, however.
  }
}
\section{Results}{
}
\section{Conclusions}{
}

\printbibliography
\end{document}
